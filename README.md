Aqui est√° o texto formatado em Markdown para o seu `README.md`.

-----

# LibraSign

## √çndice

## √çndice

* [Introdu√ß√£o](#introducao)
* [Fundamenta√ß√£o Acad√™mica](#fundamentacao-academica)
* [Vis√£o Geral do Sistema](#visao-geral-do-sistema)
    * [Escopo e Limita√ß√µes](#escopo-e-limitacoes)
    * [Arquitetura do Sistema](#arquitetura-do-sistema)
    * [Fluxo de Processamento](#fluxo-de-processamento)
* [Requisitos do Sistema](#requisitos-do-sistema)
    * [Requisitos de Software](#requisitos-de-software)
    * [Requisitos de Hardware](#requisitos-de-hardware)
* [Guia de Instala√ß√£o](#guia-de-instalacao)
    * [Etapa 1: Prepara√ß√£o do Ambiente](#etapa-1-preparacao-do-ambiente)
    * [Etapa 2: Clonagem do Reposit√≥rio](#etapa-2-clonagem-do-repositorio)
    * [Etapa 3: Configura√ß√£o do Ambiente Virtual](#etapa-3-configuracao-do-ambiente-virtual)
    * [Etapa 4: Instala√ß√£o das Depend√™ncias](#etapa-4-instalacao-das-dependencias)
* [Execu√ß√£o do Sistema](#execucao-do-sistema)
    * [Modo de Uso Padr√£o](#modo-de-uso-padrao)
    * [Captura de Novo Dataset (Opcional)](#captura-de-novo-dataset-opcional)
    * [Retreinamento do Modelo](#retreinamento-do-modelo)
* [Dataset P√∫blico](#dataset-publico)
* [Solu√ß√£o de Problemas](#solucao-de-problemas)
* [Aplicabilidade e Extensibilidade](#aplicabilidade-e-extensibilidade)
* [Considera√ß√µes Finais](#consideracoes-finais)
* [Refer√™ncias e Documenta√ß√£o Complementar](#referencias-e-documentacao-complementar)

-----

## Introdu√ß√£o

O **LibraSign** √© um sistema de c√≥digo aberto desenvolvido como projeto de conclus√£o de curso (TCC) que utiliza t√©cnicas de vis√£o computacional e aprendizado de m√°quina para realizar o reconhecimento em tempo real de gestos correspondentes ao alfabeto manual da L√≠ngua Brasileira de Sinais (**Libras**). O projeto foi concebido com o objetivo de explorar metodologias de processamento de dados geom√©tricos e classifica√ß√£o neural para aplica√ß√µes de acessibilidade comunicacional.

√â fundamental compreender que o LibraSign possui um escopo deliberadamente restrito: o sistema reconhece *exclusivamente* as configura√ß√µes de m√£o correspondentes √†s letras do alfabeto manual (A a Z), n√£o sendo capaz de interpretar palavras completas, sinais compostos ou a gram√°tica espacial complexa da Libras. Esta delimita√ß√£o foi estabelecida para viabilizar uma investiga√ß√£o acad√™mica aprofundada sobre a efic√°cia de redes neurais artificiais na classifica√ß√£o de gestos est√°ticos, servindo como prova de conceito para futuras expans√µes.

O projeto destina-se primariamente ao ambiente acad√™mico e educacional, constituindo uma ferramenta de estudo sobre processamento de sinais visuais e aprendizado supervisionado. Embora funcional, o sistema n√£o foi projetado para substituir int√©rpretes profissionais ou para uso comunicacional cotidiano em larga escala, visto que a Libras envolve elementos lingu√≠sticos complexos que ultrapassam o escopo deste trabalho, incluindo express√µes faciais, movimentos corporais e estruturas gramaticais pr√≥prias.

-----

## Fundamenta√ß√£o Acad√™mica

Este projeto representa a materializa√ß√£o de uma investiga√ß√£o cient√≠fica rigorosa conduzida no √¢mbito do curso de Bacharelado em Sistemas de Informa√ß√£o. A fundamenta√ß√£o te√≥rica completa, incluindo revis√£o de literatura sobre l√≠nguas de sinais, t√©cnicas de vis√£o computacional, arquiteturas de redes neurais, metodologia experimental, an√°lise estat√≠stica dos resultados e discuss√£o sobre as implica√ß√µes sociais da tecnologia assistiva, encontra-se detalhadamente documentada no trabalho de conclus√£o de curso.

Para uma compreens√£o aprofundada dos fundamentos te√≥ricos, das decis√µes arquiteturais, dos experimentos conduzidos e das conclus√µes alcan√ßadas, recomenda-se enfaticamente a leitura do documento acad√™mico completo, dispon√≠vel neste reposit√≥rio:

**üìÑ HeitorFernandes-TCC\_BSI.pdf**

O documento acad√™mico aborda t√≥picos essenciais como a diferencia√ß√£o entre a comunica√ß√£o em l√≠nguas de sinais e a datilologia (soletra√ß√£o manual), as limita√ß√µes das abordagens baseadas em processamento de imagens brutas, a escolha por representa√ß√µes geom√©tricas de landmarks, e as m√©tricas de desempenho obtidas atrav√©s de valida√ß√£o cruzada estratificada.

-----

## Vis√£o Geral do Sistema

### Escopo e Limita√ß√µes

Antes de prosseguir com a utiliza√ß√£o do sistema, √© imperativo que o usu√°rio compreenda claramente o escopo de funcionalidade do LibraSign. O sistema foi desenvolvido especificamente para reconhecer as configura√ß√µes de m√£o est√°ticas do alfabeto manual da Libras, que correspondem √†s vinte e seis letras do alfabeto latino (A‚ÄìZ). Esta escolha metodol√≥gica foi deliberada e alinha-se com os objetivos de pesquisa do projeto.

**O que o sistema reconhece:**

  * Configura√ß√µes de m√£o correspondentes a cada uma das letras de A a Z do alfabeto manual de Libras, quando apresentadas de forma est√°tica e isolada diante da c√¢mera.

**O que o sistema N√ÉO reconhece:**

  * Palavras completas em Libras, que frequently s√£o representadas por sinais √∫nicos e n√£o pela soletra√ß√£o letra a letra.
  * Sinais compostos ou ideogr√°ficos que constituem o vocabul√°rio padr√£o da l√≠ngua.
  * Express√µes faciais, movimento corporal ou utiliza√ß√£o do espa√ßo de sinaliza√ß√£o, elementos essenciais da gram√°tica de Libras.
  * Varia√ß√µes regionais ou dialetos da l√≠ngua de sinais.
  * Transi√ß√µes din√¢micas entre letras ou gestos em movimento cont√≠nuo.

Esta delimita√ß√£o posiciona o LibraSign como uma ferramenta educacional e de pesquisa, adequada para o estudo de t√©cnicas de reconhecimento de padr√µes e para aplica√ß√µes did√°ticas de ensino do alfabeto manual, mas n√£o como um tradutor completo da l√≠ngua de sinais. O projeto estabelece fundamentos que podem ser expandidos em trabalhos futuros para incluir vocabul√°rio mais amplo e elementos lingu√≠sticos adicionais.

### Arquitetura do Sistema

O LibraSign foi arquitetado seguindo uma metodologia modular que separa claramente as responsabilidades de cada componente do sistema. Esta organiza√ß√£o facilita a manuten√ß√£o, o teste e a eventual expans√£o das funcionalidades. A arquitetura compreende tr√™s m√≥dulos principais:

  * **M√≥dulo de Captura de Dados (`src/capture.py`):** Este componente √© respons√°vel pela aquisi√ß√£o de dados de treinamento. Utilizando a biblioteca **MediaPipe** desenvolvida pelo Google, o m√≥dulo acessa a c√¢mera do dispositivo e realiza a detec√ß√£o em tempo real das m√£os presentes no campo de vis√£o. Para cada frame capturado, o MediaPipe identifica vinte e um pontos de refer√™ncia anat√¥micos (landmarks) na m√£o detectada, extraindo suas coordenadas tridimensionais (x, y, z) no espa√ßo normalizado. Estes dados geom√©tricos, ao inv√©s de imagens brutas em pixels, s√£o persistidos em arquivos CSV organizados por classe, criando um dataset leve e estruturado que facilita o processamento posterior.

  * **M√≥dulo de Treinamento (`src/train.py`):** Este componente implementa o pipeline completo de aprendizado supervisionado. Inicialmente, o m√≥dulo carrega o dataset de landmarks a partir dos arquivos CSV gerados na etapa de captura. Em seguida, aplica uma transforma√ß√£o de normaliza√ß√£o geom√©trica que torna os dados invariantes √† posi√ß√£o absoluta da m√£o no quadro e √† escala (dist√¢ncia da c√¢mera), centralizando os pontos em rela√ß√£o ao pulso e normalizando pelo comprimento caracter√≠stico da m√£o. Ap√≥s a normaliza√ß√£o, os dados s√£o padronizados utilizando o `StandardScaler` para apresentarem m√©dia zero e vari√¢ncia unit√°ria. O modelo escolhido √© um **Perceptron Multicamadas (MLP)** com duas camadas ocultas, treinado atrav√©s do algoritmo de retropropaga√ß√£o de gradientes. A avalia√ß√£o do desempenho √© conduzida atrav√©s de valida√ß√£o cruzada estratificada com cinco parti√ß√µes, garantindo estimativas robustas da capacidade de generaliza√ß√£o. Ao final, o modelo treinado, o objeto de padroniza√ß√£o e o mapeamento de classes s√£o serializados para uso na infer√™ncia.

  * **M√≥dulo de Predi√ß√£o em Tempo Real (`src/predict.py`):** Este √© o m√≥dulo de interface com o usu√°rio, respons√°vel pela aplica√ß√£o pr√°tica do modelo treinado. O componente carrega os artefatos persistidos (modelo, scaler e classes), inicializa a captura de v√≠deo e processa cada frame em tempo real. Para cada detec√ß√£o de m√£o, as coordenadas dos landmarks s√£o extra√≠das, normalizadas e padronizadas exatamente da mesma forma que durante o treinamento, garantindo a consist√™ncia dos dados de entrada. O vetor resultante √© submetido ao classificador neural, que retorna probabilidades para cada classe. O sistema implementa duas estrat√©gias de estabiliza√ß√£o: um filtro de vota√ß√£o majorit√°ria sobre os √∫ltimos dez frames para suavizar predi√ß√µes ruidosas, e um mecanismo de confirma√ß√£o temporal que exige que uma letra permane√ßa est√°vel por dois segundos antes de ser adicionada √† frase em constru√ß√£o. O resultado √© apresentado visualmente na tela, juntamente com indicadores de confian√ßa e a senten√ßa formada.

### Fluxo de Processamento

Para auxiliar na compreens√£o da opera√ß√£o do sistema, apresenta-se a seguir o fluxo sequencial de processamento desde a captura do gesto at√© a apresenta√ß√£o do resultado:

1.  **Aquisi√ß√£o do Frame:** O sistema captura continuamente frames da c√¢mera do dispositivo em tempo real, processando aproximadamente vinte quadros por segundo dependendo do hardware dispon√≠vel.
2.  **Detec√ß√£o da M√£o:** Cada frame √© processado pelo modelo de detec√ß√£o de m√£os do MediaPipe, que utiliza redes neurais convolucionais leves para identificar a presen√ßa e localiza√ß√£o de m√£os na imagem. Quando uma m√£o √© detectada com confian√ßa superior ao limiar estabelecido, o sistema prossegue para a extra√ß√£o de landmarks.
3.  **Extra√ß√£o de Landmarks:** O MediaPipe identifica vinte e um pontos anat√¥micos na m√£o detectada, correspondendo a localiza√ß√µes como a ponta de cada dedo, as articula√ß√µes metacarpofal√¢ngicas, interfal√¢ngicas proximais e distais, al√©m do pulso. Cada landmark √© representado por suas coordenadas tridimensionais normalizadas em rela√ß√£o √†s dimens√µes da imagem.
4.  **Normaliza√ß√£o Geom√©trica:** Os landmarks brutos s√£o transformados para garantir invari√¢ncia. Primeiro, todos os pontos s√£o transladados para que o pulso (landmark zero) fique na origem do sistema de coordenadas. Em seguida, calcula-se a dist√¢ncia euclidiana entre o pulso e a base do dedo m√©dio (landmark nove), utilizando esta medida como fator de escala. Todos os pontos s√£o ent√£o divididos por este fator, resultando em uma representa√ß√£o onde o tamanho e a posi√ß√£o absoluta da m√£o n√£o influenciam a classifica√ß√£o.
5.  **Padroniza√ß√£o Estat√≠stica:** Os dados normalizados geometricamente s√£o padronizados utilizando o `StandardScaler` treinado, que subtrai a m√©dia e divide pelo desvio padr√£o de cada caracter√≠stica, conforme calculado no conjunto de treinamento. Esta etapa garante que todas as dimens√µes do vetor de entrada contribuam de forma equilibrada para a decis√£o do classificador.
6.  **Classifica√ß√£o Neural:** O vetor de caracter√≠sticas padronizado √© propagado atrav√©s das camadas do MLP. A rede processa a informa√ß√£o atrav√©s de suas cento e vinte e oito unidades na primeira camada oculta, seguidas por sessenta e quatro unidades na segunda camada, aplicando fun√ß√µes de ativa√ß√£o n√£o-lineares. A camada de sa√≠da, com dimensionalidade igual ao n√∫mero de classes, produz probabilidades atrav√©s de uma fun√ß√£o softmax.
7.  **Estabiliza√ß√£o Temporal:** Para reduzir oscila√ß√µes e predi√ß√µes esp√∫rias, o sistema mant√©m um hist√≥rico das √∫ltimas dez predi√ß√µes e aplica vota√ß√£o majorit√°ria. Al√©m disso, uma letra s√≥ √© considerada confirmada se permanecer como predi√ß√£o predominante por dois segundos consecutivos, evitando que movimentos transit√≥rios sejam interpretados como gestos intencionais.
8.  **Apresenta√ß√£o dos Resultados:** O sistema renderiza sobre o frame de v√≠deo os landmarks detectados, a letra atualmente reconhecida com indica√ß√£o de confian√ßa, uma barra de progresso para confirma√ß√£o temporal e, na por√ß√£o inferior da tela, a senten√ßa formada pelas letras confirmadas at√© o momento.

-----

## Requisitos do Sistema

### Requisitos de Software

Para a correta execu√ß√£o do LibraSign, √© necess√°rio que o ambiente de desenvolvimento atenda aos seguintes requisitos de software:

  * **Sistema Operacional:** O sistema foi desenvolvido e testado em ambientes Linux (distribui√ß√µes baseadas em Debian e Fedora), macOS (vers√µes 11 e superiores) e Windows 10/11. Em teoria, qualquer sistema operacional que suporte Python e as bibliotecas necess√°rias deve ser capaz de executar o software.
  * **Interpretador Python:** √â imprescind√≠vel a instala√ß√£o do Python na vers√£o **3.11.13**, conforme especificado no desenvolvimento do projeto. Vers√µes anteriores √† 3.9 n√£o s√£o suportadas devido √† utiliza√ß√£o de recursos sint√°ticos e de biblioteca introduzidos nessas vers√µes mais recentes. Vers√µes posteriores √† 3.11.13 podem funcionar, mas n√£o foram extensivamente testadas e podem apresentar incompatibilidades com algumas depend√™ncias.
  * **Gerenciador de Pacotes pip:** A instala√ß√£o das depend√™ncias do projeto √© realizada atrav√©s do `pip`, o gerenciador de pacotes padr√£o do Python. Vers√µes recentes do Python j√° incluem o `pip`, mas √© recomend√°vel verificar sua presen√ßa e atualiza√ß√£o antes de prosseguir.
  * **C√¢mera Funcional:** O sistema requer acesso a uma c√¢mera (webcam integrada ou externa) para captura de v√≠deo em tempo real. Certifique-se de que o sistema operacional concedeu as permiss√µes necess√°rias para que aplica√ß√µes acessem a c√¢mera.
  * **Bibliotecas Python Essenciais:** As seguintes bibliotecas constituem o n√∫cleo funcional do sistema e suas vers√µes espec√≠ficas s√£o cr√≠ticas para o funcionamento adequado:

<!-- end list -->

```
# Vers√£o do Python recomendada para este projeto: 3.11.13

scikit-learn==1.7.2
numpy==2.2.6
pandas==2.3.2
opencv-python==4.12.0.88
mediapipe==0.10.14
kagglehub==0.3.13
```

### Requisitos de Hardware

Embora o LibraSign tenha sido otimizado para execu√ß√£o em hardware modesto, certos requisitos m√≠nimos devem ser atendidos para garantir desempenho adequado:

  * **Processador:** Recomenda-se um processador moderno com pelo menos dois n√∫cleos f√≠sicos operando a uma frequ√™ncia base de 2.0 GHz ou superior. Processadores Intel Core i3 de oitava gera√ß√£o ou superiores, AMD Ryzen 3 ou equivalentes s√£o adequados. O sistema foi testado com sucesso em processadores Intel Core i5 e i7, bem como em processadores ARM de dispositivos Apple Silicon.
  * **Mem√≥ria RAM:** Um m√≠nimo de 4 GB de RAM √© necess√°rio para a execu√ß√£o do sistema. No entanto, recomenda-se 8 GB ou mais para opera√ß√£o confort√°vel, especialmente durante o treinamento do modelo, que pode consumir mem√≥ria significativa dependendo do tamanho do dataset.
  * **Armazenamento:** O projeto em si ocupa aproximadamente 16.6 MB de espa√ßo em disco. O dataset p√∫blico de landmarks, quando baixado, adiciona cerca de 32.39 MB. Recomenda-se ter pelo menos 1 GB de espa√ßo livre em disco para acomodar o projeto, datasets, modelos treinados e quaisquer datasets adicionais que o usu√°rio deseje capturar.
  * **C√¢mera:** √â essencial uma c√¢mera com resolu√ß√£o m√≠nima de 640x480 pixels (VGA) e taxa de captura de pelo menos 15 frames por segundo. C√¢meras com resolu√ß√£o HD (1280x720) ou superior e taxas de 30 FPS proporcionam melhor experi√™ncia de uso. A c√¢mera deve estar posicionada de forma a capturar claramente a m√£o do usu√°rio contra um fundo relativamente uniforme, preferencialmente com boa ilumina√ß√£o ambiente.
  * **Ilumina√ß√£o:** Embora n√£o seja um requisito de hardware per se, condi√ß√µes adequadas de ilumina√ß√£o s√£o cruciais para o desempenho do sistema. Recomenda-se ambiente bem iluminado, preferencialmente com luz natural difusa ou ilumina√ß√£o artificial uniforme, evitando contraluz intenso ou sombras fortes que possam dificultar a detec√ß√£o dos landmarks pela biblioteca MediaPipe.
  * **Sistema Gr√°fico:** Embora o sistema n√£o exija GPU dedicada, √© necess√°rio suporte b√°sico para exibi√ß√£o de janelas gr√°ficas e renderiza√ß√£o de v√≠deo. Em sistemas Linux, certifique-se de que o servidor X (X11) ou Wayland esteja configurado corretamente. Em ambientes de servidor ou cont√™ineres sem interface gr√°fica, o sistema n√£o funcionar√° adequadamente.

-----

## Guia de Instala√ß√£o

Este guia conduzir√° o usu√°rio atrav√©s de todas as etapas necess√°rias para preparar o ambiente de desenvolvimento e instalar o LibraSign em seu sistema. As instru√ß√µes s√£o apresentadas de forma detalhada e incluem comandos espec√≠ficos para diferentes sistemas operacionais quando aplic√°vel.

### Etapa 1: Prepara√ß√£o do Ambiente

Antes de iniciar a instala√ß√£o do LibraSign propriamente dito, √© necess√°rio garantir que o interpretador Python esteja instalado e configurado corretamente no sistema.

**Verifica√ß√£o da Instala√ß√£o do Python:**

Primeiramente, verifique se o Python est√° instalado no sistema e qual vers√£o est√° dispon√≠vel. Abra o terminal (Linux/macOS) ou Prompt de Comando/PowerShell (Windows) e execute o seguinte comando:

```bash
python --version
```

Em alguns sistemas, especialmente Linux e macOS, pode ser necess√°rio utilizar explicitamente o comando `python3`:

```bash
python3 --version
```

O comando deve retornar a vers√£o do Python instalada, idealmente `Python 3.11.13` ou uma vers√£o 3.11.x. Se a vers√£o retornada for inferior a 3.9, ser√° necess√°rio atualizar o Python. Se o comando n√£o for reconhecido, o Python n√£o est√° instalado ou n√£o est√° configurado corretamente no PATH do sistema.

**Instala√ß√£o do Python (se necess√°rio):**

Caso o Python n√£o esteja instalado, siga as instru√ß√µes espec√≠ficas para seu sistema operacional:

  * **Linux (Debian/Ubuntu):**
    ```bash
    sudo apt update
    sudo apt install python3.11 python3.11-venv python3-pip
    ```
  * **Linux (Fedora):**
    ```bash
    sudo dnf install python3.11
    ```
  * **macOS:** Recomenda-se utilizar o Homebrew para instala√ß√£o:
    ```bash
    brew install python@3.11
    ```
  * **Windows:** Baixe o instalador oficial do Python 3.11 atrav√©s do site `python.org`, certificando-se de marcar a op√ß√£o "**Add Python to PATH**" durante a instala√ß√£o.

**Verifica√ß√£o do pip:**

O `pip` deve ser instalado automaticamente junto com o Python. Verifique sua presen√ßa e vers√£o:

```bash
python -m pip --version
```

ou

```bash
python3 -m pip --version
```

Se o `pip` n√£o estiver dispon√≠vel, ele pode ser instalado atrav√©s do script `get-pip.py` dispon√≠vel no site oficial do Python.

### Etapa 2: Clonagem do Reposit√≥rio

Com o Python devidamente instalado, o pr√≥ximo passo consiste em obter uma c√≥pia local do reposit√≥rio do LibraSign. Certifique-se de ter o **Git** instalado no sistema antes de prosseguir.

**Verifica√ß√£o do Git:**

```bash
git --version
```

Se o Git n√£o estiver instalado, visite `git-scm.com` e siga as instru√ß√µes para seu sistema operacional.

**Clonagem do Reposit√≥rio:**

Navegue at√© o diret√≥rio onde deseja armazenar o projeto e execute o seguinte comando para clonar o reposit√≥rio:

```bash
git clone https://github.com/heitorccf/librasign.git
```

Aguarde enquanto o Git baixa todos os arquivos do reposit√≥rio. Ao concluir, um novo diret√≥rio chamado `librasign` ser√° criado contendo todos os arquivos do projeto.

**Navega√ß√£o at√© o Diret√≥rio do Projeto:**

Entre no diret√≥rio rec√©m-criado:

```bash
cd librasign
```

Todos os comandos subsequentes devem ser executados a partir deste diret√≥rio raiz do projeto.

### Etapa 3: Configura√ß√£o do Ambiente Virtual

A utiliza√ß√£o de um ambiente virtual Python √© uma pr√°tica altamente recomendada e considerada essencial para o desenvolvimento de projetos Python. O ambiente virtual cria um contexto isolado onde as depend√™ncias do projeto podem ser instaladas sem interferir com outros projetos ou com as bibliotecas do sistema. Esta abordagem previne conflitos de vers√£o e facilita a reprodutibilidade do ambiente de execu√ß√£o.

**Cria√ß√£o do Ambiente Virtual:**

  * **Linux e macOS:**
    ```bash
    python3 -m venv .venv
    ```
  * **Windows:**
    ```bash
    python -m venv .venv
    ```

Este comando cria um diret√≥rio chamado `.venv` dentro do diret√≥rio do projeto, contendo uma c√≥pia isolada do interpretador Python e ferramentas associadas. O ponto inicial no nome (`.venv`) √© uma conven√ß√£o que indica um diret√≥rio oculto ou de configura√ß√£o.

**Ativa√ß√£o do Ambiente Virtual:**

Ap√≥s criar o ambiente virtual, √© necess√°rio ativ√°-lo. O comando de ativa√ß√£o varia conforme o sistema operacional:

  * **Linux e macOS:**
    ```bash
    source .venv/bin/activate
    ```
  * **Windows (Prompt de Comando):**
    ```bash
    .venv\Scripts\activate.bat
    ```
  * **Windows (PowerShell):**
    ```bash
    .venv\Scripts\Activate.ps1
    ```

*Nota importante para usu√°rios do Windows PowerShell:* Caso encontre um erro relacionado √† pol√≠tica de execu√ß√£o de scripts, voc√™ pode precisar alterar temporariamente a pol√≠tica executando o PowerShell como administrador e digitando:

```bash
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

Ap√≥s a ativa√ß√£o bem-sucedida, voc√™ notar√° que o prompt do terminal foi modificado, exibindo o nome do ambiente virtual entre par√™nteses, geralmente `(.venv)` antes do caminho do diret√≥rio. Esta modifica√ß√£o visual confirma que o ambiente virtual est√° ativo e que quaisquer pacotes instalados via `pip` ser√£o instalados neste ambiente isolado.

**Desativa√ß√£o do Ambiente Virtual (para refer√™ncia futura):**

Embora n√£o seja necess√°rio desativar o ambiente imediatamente, √© √∫til saber que, quando desejar sair do ambiente virtual, basta executar:

```bash
deactivate
```

Este comando funciona em todos os sistemas operacionais e retorna o terminal ao ambiente Python global do sistema.

### Etapa 4: Instala√ß√£o das Depend√™ncias

Com o ambiente virtual ativado, procede-se √† instala√ß√£o de todas as bibliotecas necess√°rias para a execu√ß√£o do LibraSign. O projeto inclui um arquivo `requirements.txt` que lista todas as depend√™ncias com suas vers√µes espec√≠ficas, facilitando a instala√ß√£o em uma √∫nica opera√ß√£o.

**Atualiza√ß√£o do pip (recomendado):**

Antes de instalar as depend√™ncias, √© prudente garantir que o `pip` est√° atualizado para sua vers√£o mais recente:

```bash
python -m pip install --upgrade pip
```

**Instala√ß√£o das Depend√™ncias:**

Execute o seguinte comando para instalar todas as bibliotecas listadas no arquivo `requirements.txt`:

```bash
pip install -r requirements.txt
```

O `pip` processar√° o arquivo, resolver√° as depend√™ncias, baixar√° os pacotes necess√°rios dos reposit√≥rios oficiais do Python Package Index (PyPI) e os instalar√° no ambiente virtual. Este processo pode levar alguns minutos dependendo da velocidade da conex√£o com a internet e do poder de processamento do sistema.

Durante a instala√ß√£o, voc√™ ver√° mensagens indicando o progresso do download e instala√ß√£o de cada pacote. √â normal que alguns pacotes apresentem mensagens de compila√ß√£o, especialmente em sistemas Linux, caso seja necess√°rio compilar extens√µes em C. Aguarde at√© que o processo seja conclu√≠do completamente.

**Verifica√ß√£o da Instala√ß√£o:**

Ap√≥s a conclus√£o da instala√ß√£o, √© recomend√°vel verificar se as principais bibliotecas foram instaladas corretamente. Voc√™ pode listar todas as bibliotecas instaladas com:

```bash
pip list
```

Procure na lista as bibliotecas essenciais: `scikit-learn`, `numpy`, `pandas`, `opencv-python`, `mediapipe` e `kagglehub`. Se todas estiverem presentes, a instala√ß√£o foi bem-sucedida.

Alternativamente, voc√™ pode testar a importa√ß√£o das bibliotecas diretamente no interpretador Python:

```bash
python -c "import cv2, mediapipe, sklearn, numpy, pandas; print('Todas as bibliotecas foram importadas com sucesso')"
```

Se o comando for executado sem erros e exibir a mensagem de sucesso, o ambiente est√° corretamente configurado e pronto para uso.

-----

## Execu√ß√£o do Sistema

Uma vez que o ambiente est√° devidamente preparado e todas as depend√™ncias foram instaladas, o usu√°rio pode proceder √† execu√ß√£o do LibraSign. √â importante compreender que o sistema oferece diferentes modos de opera√ß√£o, cada um atendendo a prop√≥sitos espec√≠ficos.

### Modo de Uso Padr√£o

Para a maioria dos usu√°rios interessados em experimentar o sistema de reconhecimento de gestos sem a necessidade de treinar um novo modelo, o modo de uso padr√£o √© o mais apropriado. Este modo utiliza um modelo pr√©-treinado que foi desenvolvido com o dataset p√∫blico dispon√≠vel no Kaggle.

**Execu√ß√£o do Tradutor em Tempo Real:**

Com o ambiente virtual ativado, execute o script de predi√ß√£o:

```bash
python src/predict.py
```

Ao executar este comando, o sistema realizar√° as seguintes opera√ß√µes:

  * Primeiro, carregar√° os artefatos do modelo treinado a partir do diret√≥rio `models/`, incluindo o classificador MLP serializado, o objeto `StandardScaler` utilizado para padroniza√ß√£o dos dados e o mapeamento de classes que relaciona os √≠ndices num√©ricos √†s letras do alfabeto.
  * Em seguida, inicializar√° a biblioteca MediaPipe para detec√ß√£o de m√£os e configurar√° os par√¢metros de confian√ßa para detec√ß√£o e rastreamento.
  * Finalmente, abrir√° uma janela gr√°fica exibindo o feed de v√≠deo da c√¢mera do dispositivo, com sobreposi√ß√£o dos landmarks detectados, a letra atualmente reconhecida e a frase em constru√ß√£o.

**Instru√ß√µes de Uso Durante a Execu√ß√£o:**

1.  Posicione sua m√£o dominante no campo de vis√£o da c√¢mera, a uma dist√¢ncia aproximada de trinta a sessenta cent√≠metros, contra um fundo relativamente uniforme. A ilumina√ß√£o adequada √© fundamental para a detec√ß√£o correta dos landmarks.
2.  Forme com a m√£o a configura√ß√£o correspondente a uma letra do alfabeto manual de Libras. Mantenha a posi√ß√£o est√°vel e aguarde enquanto o sistema processa os frames. Voc√™ observar√° uma barra de progresso verde na tela que indica o tempo restante para confirma√ß√£o da letra.
3.  Ap√≥s dois segundos com a mesma letra sendo detectada consistentemente, ela ser√° adicionada √† frase exibida na parte inferior da tela. Continue formando as letras subsequentes para construir palavras ou frases.

**Controles do Teclado:**

Durante a execu√ß√£o do sistema, as seguintes teclas de controle est√£o dispon√≠veis:

  * **ESC (Escape):** Encerra a aplica√ß√£o e fecha a janela de v√≠deo.
  * **Backspace:** Remove a √∫ltima letra adicionada √† frase, permitindo corre√ß√£o de erros.
  * **C (letra c√™):** Limpa completamente a frase em constru√ß√£o, permitindo recome√ßar.

**Encerramento do Sistema:**

Para encerrar o sistema adequadamente, pressione a tecla **ESC**. O script liberar√° os recursos da c√¢mera e fechar√° todas as janelas gr√°ficas abertas, retornando ao prompt do terminal.

### Captura de Novo Dataset (Opcional)

Usu√°rios interessados em experimentar com diferentes conjuntos de dados, em expandir o sistema para reconhecer gestos adicionais ou em coletar dados espec√≠ficos de suas pr√≥prias configura√ß√µes de m√£o podem utilizar o script de captura para gerar um dataset personalizado.

**Execu√ß√£o do Script de Captura:**

Com o ambiente virtual ativado, execute:

```bash
python src/capture.py
```

O sistema abrir√° uma janela de v√≠deo e aguardar√° instru√ß√µes do usu√°rio atrav√©s do teclado.

**Processo de Captura de Dados:**

O script permite que voc√™ capture amostras organizadas por classe (letra). Para iniciar a captura de uma letra espec√≠fica:

1.  Pressione a tecla correspondente √† letra que deseja capturar (A a Z). O sistema iniciar√° imediatamente a captura autom√°tica de landmarks sempre que uma m√£o for detectada.
2.  Forme a configura√ß√£o de m√£o correspondente √† letra escolhida e mantenha-a relativamente est√°vel enquanto movimenta levemente a m√£o, alterando sutilmente sua posi√ß√£o, rota√ß√£o e dist√¢ncia da c√¢mera. Esta varia√ß√£o √© importante para que o modelo aprenda a reconhecer a letra em diferentes condi√ß√µes.
3.  O sistema capturar√° automaticamente at√© mil amostras para cada letra, salvando os dados no diret√≥rio `data/landmarks/` em arquivos CSV nomeados conforme a letra (por exemplo, `A.csv`, `B.csv`).
4.  A tela exibir√° o progresso da captura, indicando quantas amostras j√° foram coletadas do total de mil.

**Controles Durante a Captura:**

  * **A-Z:** Inicia ou alterna a captura para a letra pressionada.
  * **0 (zero):** Captura amostras da classe "nenhum", representando quadros onde nenhuma letra espec√≠fica est√° sendo formada.
  * **Espa√ßo:** Pausa ou retoma a captura para a classe atual.
  * **ESC:** Encerra o script de captura.

**Considera√ß√µes Importantes:**

  * Para obter um modelo robusto, √© fundamental capturar amostras com variabilidade adequada. Varie a ilumina√ß√£o, o √¢ngulo da c√¢mera, a rota√ß√£o da m√£o e a dist√¢ncia durante a captura. Capture amostras de diferentes pessoas se poss√≠vel, pois isso aumenta a capacidade de generaliza√ß√£o do modelo.
  * Certifique-se de formar corretamente cada configura√ß√£o de m√£o conforme o alfabeto manual de Libras. Configura√ß√µes incorretas durante a captura resultar√£o em dados rotulados erroneamente, prejudicando significativamente o desempenho do modelo treinado.

### Retreinamento do Modelo

Ap√≥s capturar um dataset personalizado ou modificado, √© necess√°rio treinar um novo modelo neural para incorporar esses dados.

**Execu√ß√£o do Script de Treinamento:**

Com o ambiente virtual ativado, execute:

```bash
python src/train.py
```

O script realizar√° as seguintes opera√ß√µes de forma automatizada:

1.  Primeiramente, baixar√° o dataset p√∫blico de refer√™ncia a partir do Kaggle utilizando a biblioteca `kagglehub`. Este dataset serve como base de treinamento padr√£o.
2.  Em seguida, carregar√° todos os arquivos CSV do diret√≥rio de landmarks, construindo as matrizes de caracter√≠sticas e vetores de r√≥tulos.
3.  Aplicar√° a normaliza√ß√£o geom√©trica aos landmarks, tornando os dados invariantes √† posi√ß√£o e escala da m√£o.
4.  Executar√° o processo de valida√ß√£o cruzada estratificada com cinco parti√ß√µes, treinando e avaliando o modelo MLP em cada parti√ß√£o, reportando a acur√°cia obtida.
5.  Calcular√° e exibir√° a acur√°cia m√©dia e o desvio padr√£o atrav√©s das parti√ß√µes, fornecendo uma estimativa robusta do desempenho esperado.
6.  Gerar√° e salvar√° uma matriz de confus√£o que detalha os padr√µes de acerto e erro do classificador.
7.  Finalmente, treinar√° um modelo definitivo utilizando todos os dados dispon√≠veis e salvar√° os artefatos (modelo, scaler e classes) no diret√≥rio `models/`.

**Interpreta√ß√£o dos Resultados:**

Durante o treinamento, o sistema exibir√° mensagens indicando o progresso e os resultados de cada fold da valida√ß√£o cruzada. Preste aten√ß√£o √† acur√°cia reportada, que idealmente deve estar acima de noventa por cento para um desempenho satisfat√≥rio na aplica√ß√£o pr√°tica.

A matriz de confus√£o salva pode ser analisada posteriormente para identificar quais letras o modelo confunde com maior frequ√™ncia, informando poss√≠veis melhorias no dataset ou na arquitetura do modelo.

**Dura√ß√£o do Treinamento:**

O tempo necess√°rio para o treinamento completo varia conforme o tamanho do dataset e a capacidade de processamento do hardware. Em um computador com processador moderno, o treinamento t√≠pico com o dataset padr√£o (aproximadamente vinte e sete mil amostras) leva entre dois e cinco minutos. Datasets maiores ou hardware mais modesto podem requerer tempo adicional.

-----

## Dataset P√∫blico

Como parte do compromisso com a ci√™ncia aberta e a reprodutibilidade da pesquisa, o dataset de landmarks utilizado no desenvolvimento e treinamento do LibraSign foi disponibilizado publicamente na plataforma Kaggle. Este dataset cont√©m aproximadamente mil amostras para cada uma das vinte e sete classes (vinte e seis letras mais a classe "nenhum"), totalizando cerca de vinte e sete mil exemplos de configura√ß√µes de m√£o.

O dataset pode ser acessado, visualizado e baixado atrav√©s do seguinte link:

üîó **[Libras Landmark Dataset (A-Z) no Kaggle](https://www.kaggle.com/datasets/heitorccf/librasign)**

**Estrutura do Dataset:**

O dataset consiste em arquivos CSV, um para cada classe, onde cada linha representa uma amostra individual contendo sessenta e tr√™s valores num√©ricos de ponto flutuante. Estes valores correspondem √†s coordenadas `x`, `y` e `z` dos vinte e um landmarks extra√≠dos pelo MediaPipe, organizados sequencialmente (`x‚ÇÄ, y‚ÇÄ, z‚ÇÄ, x‚ÇÅ, y‚ÇÅ, z‚ÇÅ, ..., x‚ÇÇ‚ÇÄ, y‚ÇÇ‚ÇÄ, z‚ÇÇ‚ÇÄ`).

**Utiliza√ß√£o do Dataset:**

Pesquisadores e desenvolvedores interessados em trabalhos relacionados podem utilizar este dataset para:

  * Reproduzir os resultados apresentados no trabalho de conclus√£o de curso.
  * Explorar diferentes arquiteturas de redes neurais e t√©cnicas de classifica√ß√£o.
  * Desenvolver sistemas de reconhecimento de gestos baseados em landmarks.
  * Realizar an√°lises comparativas de desempenho entre diferentes abordagens metodol√≥gicas.
  * Expandir o sistema com classes adicionais ou datasets complementares.

Ao utilizar este dataset em trabalhos acad√™micos ou projetos, solicita-se a devida cita√ß√£o conforme as pr√°ticas acad√™micas estabelecidas.

-----

## Solu√ß√£o de Problemas

Durante a instala√ß√£o ou execu√ß√£o do LibraSign, alguns problemas podem ser encontrados dependendo das especificidades do sistema operacional, da configura√ß√£o do hardware ou de varia√ß√µes no ambiente de software. Esta se√ß√£o documenta os problemas mais comuns e suas respectivas solu√ß√µes.

**Problema: Comando `python` n√£o reconhecido ou vers√£o incorreta do Python**

  * *Sintomas:* Ao executar `python --version`, o terminal retorna um erro indicando que o comando n√£o foi encontrado, ou retorna uma vers√£o do Python 2.x.
  * *Causa:* Em muitos sistemas Unix-like (Linux e macOS), o comando `python` aponta para o Python 2.x por raz√µes de compatibilidade hist√≥rica, enquanto o Python 3.x deve ser invocado explicitamente atrav√©s do comando `python3`.
  * *Solu√ß√£o:* Em todos os comandos apresentados neste guia onde aparece `python`, substitua por `python3`. Por exemplo, ao inv√©s de `python src/predict.py`, utilize `python3 src/predict.py`. Alternativamente, voc√™ pode criar um alias em seu shell ou modificar as vari√°veis de ambiente do sistema para que o comando `python` aponte para o Python 3.

**Problema: Erro "Permission denied" ao tentar acessar a c√¢mera**

  * *Sintomas:* O sistema inicia mas n√£o consegue abrir a c√¢mera, exibindo a mensagem "[ERRO] N√£o foi poss√≠vel abrir a c√¢mera, verifique a conex√£o e as permiss√µes".
  * *Causa:* O sistema operacional est√° bloqueando o acesso da aplica√ß√£o √† c√¢mera por quest√µes de privacidade e seguran√ßa.
  * *Solu√ß√£o no macOS:* Navegue at√© *Prefer√™ncias do Sistema \> Seguran√ßa e Privacidade \> Privacidade \> C√¢mera*, e certifique-se de que o *Terminal* (ou o aplicativo atrav√©s do qual voc√™ est√° executando o Python) tem permiss√£o para acessar a c√¢mera.
  * *Solu√ß√£o no Windows:* V√° para *Configura√ß√µes \> Privacidade \> C√¢mera*, e verifique se "Permitir que aplicativos acessem sua c√¢mera" est√° ativado. Certifique-se tamb√©m de que "Aplicativos da √°rea de trabalho" tem permiss√£o.
  * *Solu√ß√£o no Linux:* Verifique se seu usu√°rio pertence ao grupo `video`. Execute `groups` no terminal e verifique se `video` est√° listado. Se n√£o estiver, adicione seu usu√°rio ao grupo com `sudo usermod -a -G video $USER` e reinicie a sess√£o.

**Problema: `ImportError` ao tentar importar `cv2`, `mediapipe` ou outras bibliotecas**

  * *Sintomas:* Ao executar qualquer script, o Python retorna um erro similar a "ModuleNotFoundError: No module named 'cv2'" ou similar para outras bibliotecas.
  * *Causa:* As depend√™ncias n√£o foram instaladas corretamente no ambiente virtual, ou o ambiente virtual n√£o est√° ativado.
  * *Solu√ß√£o:* Primeiro, certifique-se de que o ambiente virtual est√° ativado verificando se h√° o prefixo `(.venv)` no prompt do terminal. Se n√£o estiver ativado, execute o comando de ativa√ß√£o apropriado para seu sistema. Em seguida, execute novamente `pip install -r requirements.txt` para garantir que todas as depend√™ncias sejam instaladas. Se o problema persistir, tente desinstalar e reinstalar a biblioteca problem√°tica especificamente, por exemplo: `pip uninstall opencv-python` seguido de `pip install opencv-python==4.12.0.88`.

**Problema: MediaPipe n√£o detecta a m√£o ou detec√ß√£o √© inst√°vel**

  * *Sintomas:* O sistema executa, mas os landmarks da m√£o n√£o s√£o desenhados na tela, ou a detec√ß√£o √© intermitente e inst√°vel.
  * *Causa:* Condi√ß√µes inadequadas de ilumina√ß√£o, fundo muito complexo ou confuso, ou a m√£o est√° muito pr√≥xima ou muito distante da c√¢mera.
  * *Solu√ß√£o:* Melhore a ilumina√ß√£o do ambiente, preferencialmente utilizando luz natural difusa ou ilumina√ß√£o artificial uniforme. Posicione-se contra um fundo simples e de cor relativamente uniforme, evitando padr√µes complexos ou elementos que possam ser confundidos com a m√£o. Ajuste a dist√¢ncia entre sua m√£o e a c√¢mera, experimentando posi√ß√µes entre trinta e sessenta cent√≠metros. Certifique-se de que sua m√£o est√° completamente vis√≠vel no quadro, n√£o sendo cortada pelas bordas da imagem.

**Problema: Modelo apresenta acur√°cia baixa ou predi√ß√µes inconsistentes**

  * *Sintomas:* Durante o uso do sistema em tempo real, as predi√ß√µes parecem aleat√≥rias ou frequentemente incorretas, ou durante o treinamento a acur√°cia reportada √© significativamente inferior a noventa por cento.
  * *Causa:* Dataset de treinamento com problemas, como dados rotulados incorretamente, insuficiente variabilidade nas amostras ou quantidade inadequada de exemplos por classe.
  * *Solu√ß√£o:* Revise o processo de captura do dataset, certificando-se de que as configura√ß√µes de m√£o est√£o corretas conforme o alfabeto manual de Libras. Capture mais amostras para cada classe, garantindo variabilidade em termos de ilumina√ß√£o, √¢ngulo, rota√ß√£o e dist√¢ncia da c√¢mera. Considere envolver m√∫ltiplas pessoas na captura de dados para aumentar a diversidade. Se estiver utilizando o dataset p√∫blico, verifique se o download foi completo e se os arquivos n√£o est√£o corrompidos.

**Problema: O script de treinamento falha ao baixar o dataset do Kaggle**

  * *Sintomas:* Durante a execu√ß√£o de `train.py`, o sistema reporta erros relacionados ao `kagglehub` ou falha ao baixar o dataset.
  * *Causa:* Problemas de conectividade com a internet, firewall bloqueando a conex√£o, ou configura√ß√£o inadequada das credenciais do Kaggle.
  * *Solu√ß√£o:* Verifique sua conex√£o com a internet e tente novamente. Se estiver atr√°s de um firewall corporativo, pode ser necess√°rio configurar proxies. Alternativamente, voc√™ pode baixar o dataset manualmente atrav√©s do link do Kaggle, descompactar e colocar os arquivos CSV no diret√≥rio `data/landmarks/`, em seguida modificar o script `train.py` para n√£o executar o download autom√°tico (comentando a se√ß√£o de download do `kagglehub`).

**Problema: No Windows, erro "cannot be loaded because running scripts is disabled on this system" ao ativar o ambiente virtual**

  * *Sintomas:* Ao tentar ativar o ambiente virtual no PowerShell, aparece uma mensagem de erro relacionada √† pol√≠tica de execu√ß√£o de scripts.
  * *Causa:* O PowerShell tem uma pol√≠tica de seguran√ßa que por padr√£o impede a execu√ß√£o de scripts n√£o assinados.
  * *Solu√ß√£o:* Abra o PowerShell como administrador e execute `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`. Isso permitir√° a execu√ß√£o de scripts locais n√£o assinados. Ap√≥s essa configura√ß√£o, tente ativar o ambiente virtual novamente. Como alternativa, voc√™ pode utilizar o Prompt de Comando (`cmd.exe`) ao inv√©s do PowerShell, onde esta restri√ß√£o n√£o se aplica.

**Problema: Janela do OpenCV n√£o abre ou congela em ambientes Linux sem GUI**

  * *Sintomas:* Em servidores Linux ou ambientes sem interface gr√°fica (como SSH sem X forwarding), o sistema falha ao tentar abrir a janela de v√≠deo.
  * *Causa:* O LibraSign foi projetado para ambientes com interface gr√°fica completa, utilizando janelas do OpenCV para exibi√ß√£o do v√≠deo.
  * *Solu√ß√£o:* O sistema n√£o √© adequado para execu√ß√£o em ambientes sem GUI. Para utiliza√ß√£o remota, considere configurar X11 forwarding em sua conex√£o SSH (`ssh -X` ou `ssh -Y`), ou utilize solu√ß√µes de desktop remoto como VNC. Alternativamente, o c√≥digo poderia ser modificado para salvar frames processados em disco ao inv√©s de exibi-los, mas isso est√° al√©m do escopo do uso padr√£o do sistema.

**Problema: Desempenho lento, frames travando ou baixa taxa de atualiza√ß√£o**

  * *Sintomas:* O v√≠deo na janela do sistema atualiza de forma muito lenta, apresenta travamentos frequentes, ou a predi√ß√£o demora muito tempo.
  * *Causa:* Hardware insuficiente, outros processos consumindo recursos do sistema, ou c√¢mera de baixa qualidade.
  * *Solu√ß√£o:* Feche outros aplicativos que possam estar consumindo CPU ou RAM significativos. Reduza a resolu√ß√£o da c√¢mera se poss√≠vel. Em sistemas Linux, considere fechar aplicativos pesados de desktop. Certifique-se de que drivers de v√≠deo est√£o atualizados. Como √∫ltimo recurso, considere utilizar um computador com especifica√ß√µes mais robustas.

-----

## Aplicabilidade e Extensibilidade

Embora o LibraSign tenha sido desenvolvido especificamente para o reconhecimento do alfabeto manual da L√≠ngua Brasileira de Sinais, sua arquitetura modular e metodologia baseada em landmarks geom√©tricos conferem ao sistema not√°vel flexibilidade e potencial para adapta√ß√£o a contextos diversos.

  * **Adapta√ß√£o para Outras L√≠nguas de Sinais:** A estrutura do sistema pode ser prontamente retreinada para reconhecer alfabetos manuais de outras l√≠nguas de sinais nacionais, como a American Sign Language (ASL), a British Sign Language (BSL), ou qualquer outra l√≠ngua de sinais que utilize datilologia. O processo requer apenas a captura de um novo dataset com as configura√ß√µes de m√£o espec√≠ficas da l√≠ngua-alvo, seguido do retreinamento do modelo conforme descrito neste guia.

  * **Expans√£o para Vocabul√°rio Mais Amplo:** Pesquisadores interessados em expandir o sistema al√©m do alfabeto manual podem coletar amostras de sinais ideogr√°ficos completos (palavras em Libras) e inclu√≠-los como classes adicionais no dataset. Esta expans√£o demandaria possivelmente arquiteturas de rede mais complexas, capazes de modelar sequ√™ncias temporais, como Redes Neurais Recorrentes (RNN) ou Transformers, visto que muitos sinais envolvem movimento din√¢mico das m√£os.

  * **Reconhecimento de Gestos Personalizados:** A metodologia pode ser aplicada para reconhecer conjuntos de gestos personalizados em contextos diversos, como controle gestual de interfaces, interpreta√ß√£o de comandos em ambientes de realidade virtual ou aumentada, ou sistemas de comunica√ß√£o customizados para necessidades espec√≠ficas. A versatilidade dos landmarks do MediaPipe permite que praticamente qualquer configura√ß√£o de m√£o distingu√≠vel seja capturada e classificada.

  * **Integra√ß√£o com Outras Modalidades:** O sistema atual processa exclusivamente a configura√ß√£o espacial da m√£o. Trabalhos futuros poderiam integrar informa√ß√£o facial (express√µes), corporal (postura e orienta√ß√£o), e contextual (posi√ß√£o no espa√ßo de sinaliza√ß√£o) para aproximar-se de um sistema de reconhecimento mais completo da l√≠ngua de sinais em sua riqueza lingu√≠stica.

  * **Aplica√ß√µes Educacionais:** O LibraSign serve como ferramenta did√°tica valiosa para o ensino de conceitos de aprendizado de m√°quina, vis√£o computacional e processamento de sinais. Estudantes podem experimentar com diferentes arquiteturas de rede, t√©cnicas de pr√©-processamento, estrat√©gias de aumento de dados e metodologias de valida√ß√£o, utilizando o sistema como plataforma de aprendizado pr√°tico.

-----

## Considera√ß√µes Finais

O LibraSign representa uma contribui√ß√£o ao campo da tecnologia assistiva e ao reconhecimento automatizado de l√≠nguas de sinais, demonstrando a viabilidade de abordagens baseadas em landmarks geom√©tricos para a classifica√ß√£o de gestos manuais. O projeto foi desenvolvido com rigor acad√™mico, aten√ß√£o √† reprodutibilidade e compromisso com a dissemina√ß√£o do conhecimento atrav√©s de c√≥digo aberto e datasets p√∫blicos.

√â essencial reiterar que o sistema, em seu estado atual, possui limita√ß√µes significativas que o posicionam como ferramenta de pesquisa e educa√ß√£o, n√£o como substituto para comunica√ß√£o profissional ou interpreta√ß√£o da l√≠ngua de sinais. A Libras, assim como outras l√≠nguas de sinais, constitui um sistema lingu√≠stico completo e complexo, com gram√°tica, sintaxe, sem√¢ntica e pragm√°tica pr√≥prias que transcendem vastamente a mera soletra√ß√£o manual de letras.

O reconhecimento do alfabeto manual, embora √∫til em contextos espec√≠ficos como soletra√ß√£o de nomes pr√≥prios ou termos t√©cnicos sem sinal estabelecido, representa apenas uma fra√ß√£o diminuta da comunica√ß√£o em Libras. A compreens√£o adequada da l√≠ngua envolve express√µes faciais que modificam significado gramatical, movimento e orienta√ß√£o das m√£os no espa√ßo tridimensional de sinaliza√ß√£o, uso de classificadores, incorpora√ß√£o e referencia√ß√£o espacial, entre in√∫meros outros elementos lingu√≠sticos.

Portanto, enfatiza-se que o LibraSign n√£o deve ser interpretado como sistema de tradu√ß√£o da Libras em sua totalidade, mas sim como um primeiro passo metodol√≥gico em dire√ß√£o a sistemas mais abrangentes, e como ferramenta valiosa para o estudo de t√©cnicas de reconhecimento de padr√µes visuais.

Usu√°rios interessados em aprofundar-se na compreens√£o t√©cnica do sistema, nos fundamentos te√≥ricos que embasam as decis√µes arquiteturais, nos resultados experimentais detalhados e nas discuss√µes sobre trabalhos relacionados s√£o encorajados a consultar o documento acad√™mico completo referenciado na pr√≥xima se√ß√£o.

-----

## Refer√™ncias e Documenta√ß√£o Complementar

**üìÑ Trabalho de Conclus√£o de Curso Completo:**

  * [HeitorFernandes-TCC\_BSI.pdf](https://github.com/Heitorccf/librasign/blob/master/HeitorFernandes-TCC_BSI.pdf)

Este documento acad√™mico apresenta de forma detalhada e rigorosa todos os aspectos do projeto, incluindo:

  * Revis√£o bibliogr√°fica sobre l√≠nguas de sinais, tecnologias assistivas e reconhecimento de gestos.
  * Discuss√£o sobre abordagens metodol√≥gicas para processamento de sinais visuais.
  * Fundamenta√ß√£o te√≥rica sobre redes neurais artificiais e perceptrons multicamadas.
  * Descri√ß√£o detalhada do processo de coleta e prepara√ß√£o do dataset.
  * An√°lise estat√≠stica completa dos resultados experimentais.
  * Matrizes de confus√£o e m√©tricas de desempenho discriminadas por classe.
  * Discuss√£o sobre limita√ß√µes do sistema e considera√ß√µes para trabalhos futuros.
  * Reflex√µes sobre o impacto social da tecnologia e quest√µes √©ticas relacionadas.

**Documenta√ß√£o das Bibliotecas Utilizadas:**

Para usu√°rios interessados em compreender mais profundamente as tecnologias empregadas no LibraSign, recomenda-se a consulta da documenta√ß√£o oficial das principais bibliotecas:

  * **MediaPipe:** [https://developers.google.com/mediapipe](https://developers.google.com/mediapipe)
  * **scikit-learn:** [https://scikit-learn.org/stable/documentation.html](https://scikit-learn.org/stable/documentation.html)
  * **OpenCV:** [https://docs.opencv.org/](https://docs.opencv.org/)
  * **NumPy:** [https://numpy.org/doc/](https://numpy.org/doc/)
  * **Pandas:** [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/)

**Recursos sobre Libras:**

Para aqueles interessados em aprender mais sobre a L√≠ngua Brasileira de Sinais e sua estrutura lingu√≠stica:

  * **Instituto Nacional de Educa√ß√£o de Surdos (INES):** [http://www.ines.gov.br](http://www.ines.gov.br)
  * **Dicion√°rio de Libras do INES:** Recurso online para consulta de sinais.
  * **Federa√ß√£o Nacional de Educa√ß√£o e Integra√ß√£o dos Surdos (FENEIS):** [https://www.feneis.org.br](https://www.feneis.org.br)

**Contribui√ß√µes e Feedback:**

O LibraSign √© um projeto de c√≥digo aberto e contribui√ß√µes da comunidade s√£o bem-vindas. Usu√°rios que identificarem bugs, tiverem sugest√µes de melhorias ou desejarem contribuir com c√≥digo s√£o encorajados a abrir issues ou pull requests no reposit√≥rio do GitHub.

Para quest√µes acad√™micas, d√∫vidas t√©cnicas ou discuss√µes sobre o projeto, sinta-se √† vontade para entrar em contato atrav√©s dos canais disponibilizados no reposit√≥rio.

**Agradecimentos:**

O desenvolvimento do LibraSign foi poss√≠vel gra√ßas ao apoio institucional da universidade, √† orienta√ß√£o acad√™mica recebida, ao acesso a recursos computacionais, e √† disponibiliza√ß√£o gratuita de bibliotecas de c√≥digo aberto pela comunidade cient√≠fica e tecnol√≥gica internacional. Agradecimentos especiais √† comunidade surda brasileira, cuja l√≠ngua e cultura inspiram este trabalho e motivam o desenvolvimento de tecnologias mais inclusivas.

**Licen√ßa:**

Este projeto √© distribu√≠do sob licen√ßa de c√≥digo aberto, permitindo uso, modifica√ß√£o e distribui√ß√£o de acordo com os termos especificados no arquivo `LICENSE` do reposit√≥rio. Ao utilizar ou modificar este c√≥digo, solicita-se que a devida atribui√ß√£o seja mantida conforme as pr√°ticas da comunidade de software livre.

-----

**√öltima Atualiza√ß√£o:** Novembro de 2025

**Autor:** Heitor Fernandes
**Institui√ß√£o:** Bacharelado em Sistemas de Informa√ß√£o
**Reposit√≥rio:** [https://github.com/heitorccf/librasign](https://github.com/heitorccf/librasign)
**Dataset P√∫blico:** [https://www.kaggle.com/datasets/heitorccf/librasign](https://www.kaggle.com/datasets/heitorccf/librasign)

-----

*Este README foi elaborado com o objetivo de fornecer documenta√ß√£o abrangente e acess√≠vel para usu√°rios de diferentes n√≠veis de experi√™ncia t√©cnica. Para sugest√µes de melhorias nesta documenta√ß√£o, por favor, entre em contato atrav√©s do reposit√≥rio do GitHub.*