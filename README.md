<<<<<<< HEAD
=======
Sistema de TraduÃ§Ã£o em Tempo Real do Alfabeto da Libras (Librasign)

!(https://img.shields.io/badge/status-ConcluÃ­do-success.svg)

PortuguÃªs | 

Contato

Heitor CÃ¢mara Costa Fernandes

ðŸ‡§ðŸ‡· PortuguÃªs

Metodologia e Funcionamento âš™ï¸

O sistema opera atravÃ©s de um pipeline de processamento que se divide em trÃªs etapas fundamentais: coleta de dados, treinamento do modelo e traduÃ§Ã£o em tempo real.

1. Coleta e RepresentaÃ§Ã£o de Dados GeomÃ©tricos ðŸ–ï¸

Em vez de capturar e armazenar milhares de imagens, o sistema utiliza uma webcam para detectar a mÃ£o do usuÃ¡rio em tempo real. A biblioteca MediaPipe Ã© empregada para identificar e extrair as coordenadas 3D de 21 pontos de referÃªncia (landmarks) que compÃµem a estrutura da mÃ£o.  Cada gesto Ã©, portanto, convertido em um vetor numÃ©rico de 63 dimensÃµes (21 pontos x 3 coordenadas), representando sua geometria esquelÃ©tica. Esses vetores sÃ£o salvos em arquivos 

.csv, criando um dataset leve, preciso e imune a ruÃ­dos visuais como variaÃ§Ãµes de iluminaÃ§Ã£o ou complexidade do fundo.

2. Treinamento do Modelo de ClassificaÃ§Ã£o ðŸ§ 

Com os dados geomÃ©tricos coletados, um modelo de machine learning Ã© treinado para associar cada vetor de coordenadas a uma letra do alfabeto. Devido Ã  natureza tabular e de baixa dimensionalidade dos dados, optou-se por um modelo Perceptron de MÃºltiplas Camadas (MLP), uma arquitetura de rede neural eficiente para este tipo de tarefa. Antes do treinamento, os dados passam por um processo de normalizaÃ§Ã£o (standardization) para garantir que todas as caracterÃ­sticas contribuam de forma equitativa para o aprendizado. O resultado Ã© um modelo classificador altamente otimizado, treinado em segundos, que aprende a distinguir os gestos unicamente a partir de sua forma e estrutura.

3. TraduÃ§Ã£o em Tempo Real ðŸš€

A aplicaÃ§Ã£o final integra os componentes anteriores para fornecer uma traduÃ§Ã£o instantÃ¢nea. O sistema captura o vÃ­deo da webcam, extrai os landmarks da mÃ£o em cada quadro, aplica a mesma normalizaÃ§Ã£o utilizada no treinamento e alimenta o vetor de coordenadas ao modelo MLP treinado. O modelo, entÃ£o, prediz a qual letra o gesto corresponde, e o resultado Ã© exibido na tela para o usuÃ¡rio. Este ciclo de detecÃ§Ã£o, processamento e classificaÃ§Ã£o ocorre de forma contÃ­nua e com baixa latÃªncia, criando uma ferramenta de comunicaÃ§Ã£o interativa e funcional.

PublicaÃ§Ã£o AcadÃªmica ðŸŽ“

O documento completo do Trabalho de ConclusÃ£o de Curso, contendo a fundamentaÃ§Ã£o teÃ³rica, a metodologia detalhada e a anÃ¡lise dos resultados, estÃ¡ disponÃ­vel para visualizaÃ§Ã£o e download em repositÃ³rios acadÃªmicos permanentes.

    Zenodo (DOI): ``

    ResearchGate: ``

Tecnologias Utilizadas ðŸ› ï¸

    Linguagem: Python 3.9+

    VisÃ£o Computacional: OpenCV, MediaPipe

    Machine Learning: Scikit-learn (MLPClassifier, StandardScaler)

    ManipulaÃ§Ã£o de Dados: NumPy, Pandas

LicenÃ§a Â©ï¸

Este projeto estÃ¡ licenciado sob a GNU General Public License v3.0. Veja o arquivo LICENSE para mais detalhes.

Agradecimentos ðŸ™

    Ao Professor Orientador CecÃ­lio Merlotti Rodas, pelo suporte e direcionamento acadÃªmico.

    Ao Instituto Federal de EducaÃ§Ã£o, CiÃªncia e Tecnologia de SÃ£o Paulo (IFSP), pela estrutura e fomento Ã  pesquisa.

Este projeto foi desenvolvido com o objetivo de promover a inclusÃ£o e a acessibilidade atravÃ©s da tecnologia. Que ele possa contribuir para quebrar barreiras de comunicaÃ§Ã£o e aproximar pessoas.

ðŸ‡¬ðŸ‡§ ðŸ‡ºðŸ‡¸ English

Methodology and How It Works

The system operates through a processing pipeline divided into three fundamental stages: data collection, model training, and real-time translation.

1. Geometric Data Collection and Representation

Instead of capturing and storing thousands of images, the system uses a webcam to detect the user's hand in real-time. The MediaPipe library is employed to identify and extract the 3D coordinates of 21 reference points (landmarks) that make up the hand's structure.  Each gesture is thus converted into a 63-dimensional numerical vector (21 points x 3 coordinates), representing its skeletal geometry. These vectors are saved into 

.csv files, creating a lightweight, precise dataset that is immune to visual noise such as lighting variations or background complexity.

2. Classification Model Training

With the geometric data collected, a machine learning model is trained to associate each coordinate vector with a letter of the alphabet. Due to the tabular and low-dimensional nature of the data, a Multi-Layer Perceptron (MLP) model was chosen, an efficient neural network architecture for this type of task. Before training, the data undergoes a standardization process to ensure that all features contribute equally to the learning process. The result is a highly optimized classifier model, trained in seconds, that learns to distinguish gestures solely based on their shape and structure.

3. Real-Time Translation

The final application integrates the previous components to provide instantaneous translation. The system captures video from the webcam, extracts the hand landmarks in each frame, applies the same normalization used during training, and feeds the coordinate vector to the trained MLP model. The model then predicts which letter the gesture corresponds to, and the result is displayed on the screen for the user. This cycle of detection, processing, and classification occurs continuously and with low latency, creating an interactive and functional communication tool.

Academic Publication

The full Final Year Project document, containing the theoretical foundation, detailed methodology, and analysis of the results, is available for viewing and download in permanent academic repositories.

    Zenodo (DOI): ``

    ResearchGate: ``

Technology Stack

    Linguagem: Python 3.9+

    VisÃ£o Computacional: OpenCV, MediaPipe

    Machine Learning: Scikit-learn (MLPClassifier, StandardScaler)

    ManipulaÃ§Ã£o de Dados: NumPy, Pandas

License

This project is licensed under the GNU General Public License v3.0. See the LICENSE file for more details.

Acknowledgments

    To my advisor, Professor CecÃ­lio Merlotti Rodas, for the academic support and guidance.

    To the Federal Institute of Education, Science and Technology of SÃ£o Paulo (IFSP), for the infrastructure and encouragement of research.

This project was developed with the goal of promoting inclusion and accessibility through technology. May it contribute to breaking down communication barriers and bringing people closer together.
>>>>>>> d5eb1054a661822aafbe54037522813806705410
