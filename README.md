# LibraSign - Sistema de Reconhecimento de Libras

## üìã Sobre o Projeto

O LibraSign √© um sistema de reconhecimento de gestos da L√≠ngua Brasileira de Sinais (Libras) desenvolvido com tecnologias de vis√£o computacional e aprendizado profundo. O projeto utiliza redes neurais convolucionais para identificar e classificar gestos do alfabeto em Libras capturados atrav√©s de uma webcam em tempo real.

Este sistema foi desenvolvido com o objetivo de contribuir para a acessibilidade e inclus√£o digital, oferecendo uma ferramenta que pode auxiliar no aprendizado e na comunica√ß√£o atrav√©s da Libras. A aplica√ß√£o √© capaz de reconhecer os gestos das letras do alfabeto, processando as imagens capturadas e fornecendo feedback visual instant√¢neo ao usu√°rio.

## ‚ú® Funcionalidades Principais

O LibraSign oferece um conjunto completo de funcionalidades para captura, treinamento e reconhecimento de gestos:

**Captura de Dados**: O sistema permite a coleta sistem√°tica de imagens de gestos atrav√©s da webcam, organizando automaticamente o conjunto de dados por categoria alfab√©tica. Durante a captura, o usu√°rio pode visualizar em tempo real a detec√ß√£o da m√£o e o processo de salvamento das imagens.

**Pr√©-processamento Inteligente**: Todas as imagens capturadas passam por um pipeline de processamento que inclui convers√£o para escala de cinza, redimensionamento padronizado e normaliza√ß√£o dos valores de pixel, garantindo consist√™ncia e otimiza√ß√£o para o treinamento do modelo.

**Treinamento de Modelo**: O sistema implementa uma arquitetura de rede neural convolucional otimizada para reconhecimento de padr√µes visuais, com camadas de convolu√ß√£o, pooling e regulariza√ß√£o atrav√©s de dropout para prevenir sobreajuste.

**Reconhecimento em Tempo Real**: A aplica√ß√£o principal oferece predi√ß√£o instant√¢nea dos gestos capturados pela webcam, exibindo o resultado diretamente na interface visual com indicadores claros do estado do sistema.

## üõ†Ô∏è Tecnologias Utilizadas

O projeto foi constru√≠do utilizando um conjunto robusto de bibliotecas e frameworks modernos:

- **Python 3.8+**: Linguagem principal do projeto
- **OpenCV**: Processamento de imagens e interface com webcam
- **MediaPipe**: Detec√ß√£o e rastreamento de m√£os em tempo real
- **TensorFlow/Keras**: Constru√ß√£o e treinamento da rede neural convolucional
- **NumPy**: Manipula√ß√£o eficiente de arrays multidimensionais
- **Scikit-learn**: Ferramentas de pr√©-processamento e divis√£o de dados

## üìÅ Estrutura do Projeto

```
librasign/
‚îÇ
‚îú‚îÄ‚îÄ capture.py           # Sistema de captura de imagens via webcam
‚îú‚îÄ‚îÄ normalizing.py       # Pipeline de pr√©-processamento de dados
‚îú‚îÄ‚îÄ train.py            # Treinamento do modelo de rede neural
‚îú‚îÄ‚îÄ predict.py          # Aplica√ß√£o de reconhecimento em tempo real
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ raw/            # Diret√≥rio para armazenamento das imagens capturadas
‚îÇ       ‚îú‚îÄ‚îÄ A/          # Imagens da letra A
‚îÇ       ‚îú‚îÄ‚îÄ B/          # Imagens da letra B
‚îÇ       ‚îî‚îÄ‚îÄ ...         # Demais letras do alfabeto
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ best_model.keras  # Modelo treinado salvo
‚îÇ
‚îú‚îÄ‚îÄ LICENSE             # Licen√ßa GPL v3
‚îî‚îÄ‚îÄ README.md          # Este arquivo
```

## üìã Pr√©-requisitos

Antes de iniciar a instala√ß√£o do LibraSign, certifique-se de que seu sistema atende aos seguintes requisitos:

- Python 3.8 ou superior instalado
- Webcam funcional conectada ao computador
- Sistema operacional: Windows, Linux ou macOS
- Pelo menos 4GB de RAM dispon√≠vel
- Espa√ßo em disco: aproximadamente 500MB para o projeto e dados

## üöÄ Instala√ß√£o

Siga este passo a passo detalhado para configurar o LibraSign em seu ambiente:

### 1. Clone o reposit√≥rio

```bash
git clone https://github.com/seu-usuario/librasign.git
cd librasign
```

### 2. Crie um ambiente virtual

√â altamente recomendado utilizar um ambiente virtual para evitar conflitos entre depend√™ncias:

```bash
# No Windows
python -m venv venv
venv\Scripts\activate

# No Linux/macOS
python3 -m venv venv
source venv/bin/activate
```

### 3. Instale as depend√™ncias

```bash
pip install opencv-python mediapipe tensorflow numpy scikit-learn
```

### 4. Crie a estrutura de diret√≥rios necess√°ria

```bash
mkdir -p data/raw models
```

## üíª Como Usar

O LibraSign funciona atrav√©s de um fluxo de trabalho sequencial que voc√™ deve seguir para obter os melhores resultados:

### Etapa 1: Captura de Dados

Execute o script de captura para coletar imagens de treinamento:

```bash
python capture.py
```

Durante a execu√ß√£o, o sistema mostrar√° uma janela com o v√≠deo da webcam. Para capturar imagens de uma letra espec√≠fica, pressione a tecla correspondente (A-Z) no teclado. O sistema come√ßar√° a salvar automaticamente as imagens detectadas da sua m√£o fazendo o gesto. Recomenda-se capturar pelo menos 100 imagens por letra, variando a posi√ß√£o, ilumina√ß√£o e √¢ngulo da m√£o para criar um conjunto de dados robusto.

### Etapa 2: Processamento dos Dados

O pr√©-processamento √© executado automaticamente quando voc√™ treina o modelo, mas voc√™ pode verificar se os dados est√£o corretos executando:

```bash
python normalizing.py
```

Este script carregar√° todas as imagens capturadas, aplicar√° as transforma√ß√µes necess√°rias e exibir√° informa√ß√µes sobre o conjunto de dados, incluindo o n√∫mero total de imagens e as classes detectadas.

### Etapa 3: Treinamento do Modelo

Inicie o processo de treinamento da rede neural:

```bash
python train.py
```

O treinamento pode levar alguns minutos, dependendo da quantidade de dados e do poder de processamento do seu computador. Durante o processo, voc√™ ver√° informa√ß√µes sobre o progresso, incluindo a acur√°cia do modelo em cada √©poca. O melhor modelo ser√° salvo automaticamente no diret√≥rio `models/`.

### Etapa 4: Reconhecimento em Tempo Real

Ap√≥s o treinamento bem-sucedido, execute a aplica√ß√£o principal:

```bash
python predict.py
```

Uma janela ser√° aberta mostrando o v√≠deo da webcam com overlay de detec√ß√£o. Fa√ßa gestos de letras em Libras em frente √† c√¢mera e o sistema exibir√° a letra reconhecida em tempo real no canto superior direito da tela.

## üß† Como Funciona

O LibraSign implementa um pipeline completo de vis√£o computacional e aprendizado de m√°quina que pode ser compreendido em quatro componentes principais:

**Detec√ß√£o de M√£os**: O sistema utiliza o MediaPipe, uma biblioteca desenvolvida pelo Google, para detectar e rastrear pontos de refer√™ncia anat√¥micos da m√£o em tempo real. O MediaPipe identifica 21 pontos-chave na m√£o, permitindo o c√°lculo preciso da regi√£o de interesse (ROI) que cont√©m o gesto.

**Pr√©-processamento de Imagens**: Cada imagem capturada passa por uma s√©rie de transforma√ß√µes essenciais. Primeiro, a regi√£o da m√£o √© extra√≠da e convertida para escala de cinza, removendo informa√ß√µes de cor que s√£o irrelevantes para a forma do gesto. Em seguida, a imagem √© redimensionada para 224x224 pixels e os valores dos pixels s√£o normalizados para o intervalo [0, 1], otimizando o processo de aprendizagem da rede neural.

**Arquitetura da Rede Neural**: O modelo utiliza uma arquitetura convolucional com duas camadas de convolu√ß√£o (32 e 64 filtros), intercaladas com camadas de max pooling para redu√ß√£o dimensional. Ap√≥s o achatamento dos mapas de caracter√≠sticas, uma camada densa com 128 neur√¥nios processa as informa√ß√µes, seguida de dropout (50%) para regulariza√ß√£o. A camada de sa√≠da utiliza ativa√ß√£o softmax para gerar probabilidades para cada classe.

**Infer√™ncia e Predi√ß√£o**: Durante o reconhecimento em tempo real, cada frame capturado pela webcam passa pelo mesmo pipeline de pr√©-processamento usado no treinamento. O modelo processa a imagem e retorna um vetor de probabilidades, onde cada posi√ß√£o corresponde a uma letra do alfabeto. A letra com maior probabilidade √© selecionada como a predi√ß√£o final.

## ü§ù Contribuindo

Contribui√ß√µes s√£o extremamente bem-vindas e valorizadas! O LibraSign √© um projeto de c√≥digo aberto e sua evolu√ß√£o depende da colabora√ß√£o da comunidade. Se voc√™ deseja contribuir, siga estas diretrizes:

1. Fa√ßa um fork do projeto atrav√©s do GitHub
2. Crie uma branch para sua funcionalidade (`git checkout -b feature/NovaFuncionalidade`)
3. Commit suas mudan√ßas com mensagens descritivas (`git commit -m 'Adicionando nova funcionalidade X'`)
4. Push para a branch (`git push origin feature/NovaFuncionalidade`)
5. Abra um Pull Request detalhando as mudan√ßas propostas

Algumas √°reas onde contribui√ß√µes seriam especialmente valiosas:
- Implementa√ß√£o de reconhecimento de palavras completas
- Otimiza√ß√£o do modelo para melhor acur√°cia
- Interface gr√°fica mais elaborada
- Suporte para reconhecimento de n√∫meros e express√µes
- Documenta√ß√£o adicional e tutoriais

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a GNU General Public License v3.0 (GPL-3.0). Isso significa que voc√™ tem a liberdade de usar, modificar e distribuir este software, desde que mantenha a mesma licen√ßa e os cr√©ditos originais. Para mais detalhes, consulte o arquivo [LICENSE](LICENSE) no reposit√≥rio.

A escolha da GPL v3 reflete nosso compromisso com o software livre e a acessibilidade tecnol√≥gica, garantindo que melhorias e deriva√ß√µes deste projeto permane√ßam abertas e acess√≠veis √† comunidade.

## ‚úâÔ∏è Autor e Contato

**Heitor C√¢mara Costa Fernandes**

- Email: Heitorccfernandes550@gmail.com
- Ano de desenvolvimento: 2025

Para d√∫vidas, sugest√µes ou reportar problemas, sinta-se √† vontade para abrir uma issue no reposit√≥rio ou entrar em contato diretamente atrav√©s do email fornecido.

---

*Este projeto foi desenvolvido com o objetivo de promover a inclus√£o e acessibilidade atrav√©s da tecnologia. Que ele possa contribuir para quebrar barreiras de comunica√ß√£o e aproximar pessoas.*