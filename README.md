Sistema de Tradu√ß√£o em Tempo Real do Alfabeto da Libras (Librasign)

!(https://img.shields.io/badge/status-Conclu√≠do-success.svg)

Portugu√™s | 

Contato

Heitor C√¢mara Costa Fernandes

üáßüá∑ Portugu√™s

Metodologia e Funcionamento ‚öôÔ∏è

O sistema opera atrav√©s de um pipeline de processamento que se divide em tr√™s etapas fundamentais: coleta de dados, treinamento do modelo e tradu√ß√£o em tempo real.

1. Coleta e Representa√ß√£o de Dados Geom√©tricos üñêÔ∏è

Em vez de capturar e armazenar milhares de imagens, o sistema utiliza uma webcam para detectar a m√£o do usu√°rio em tempo real. A biblioteca MediaPipe √© empregada para identificar e extrair as coordenadas 3D de 21 pontos de refer√™ncia (landmarks) que comp√µem a estrutura da m√£o.  Cada gesto √©, portanto, convertido em um vetor num√©rico de 63 dimens√µes (21 pontos x 3 coordenadas), representando sua geometria esquel√©tica. Esses vetores s√£o salvos em arquivos 

.csv, criando um dataset leve, preciso e imune a ru√≠dos visuais como varia√ß√µes de ilumina√ß√£o ou complexidade do fundo.

2. Treinamento do Modelo de Classifica√ß√£o üß†

Com os dados geom√©tricos coletados, um modelo de machine learning √© treinado para associar cada vetor de coordenadas a uma letra do alfabeto. Devido √† natureza tabular e de baixa dimensionalidade dos dados, optou-se por um modelo Perceptron de M√∫ltiplas Camadas (MLP), uma arquitetura de rede neural eficiente para este tipo de tarefa. Antes do treinamento, os dados passam por um processo de normaliza√ß√£o (standardization) para garantir que todas as caracter√≠sticas contribuam de forma equitativa para o aprendizado. O resultado √© um modelo classificador altamente otimizado, treinado em segundos, que aprende a distinguir os gestos unicamente a partir de sua forma e estrutura.

3. Tradu√ß√£o em Tempo Real üöÄ

A aplica√ß√£o final integra os componentes anteriores para fornecer uma tradu√ß√£o instant√¢nea. O sistema captura o v√≠deo da webcam, extrai os landmarks da m√£o em cada quadro, aplica a mesma normaliza√ß√£o utilizada no treinamento e alimenta o vetor de coordenadas ao modelo MLP treinado. O modelo, ent√£o, prediz a qual letra o gesto corresponde, e o resultado √© exibido na tela para o usu√°rio. Este ciclo de detec√ß√£o, processamento e classifica√ß√£o ocorre de forma cont√≠nua e com baixa lat√™ncia, criando uma ferramenta de comunica√ß√£o interativa e funcional.

Publica√ß√£o Acad√™mica üéì

O documento completo do Trabalho de Conclus√£o de Curso, contendo a fundamenta√ß√£o te√≥rica, a metodologia detalhada e a an√°lise dos resultados, est√° dispon√≠vel para visualiza√ß√£o e download em reposit√≥rios acad√™micos permanentes.

    Zenodo (DOI): ``

    ResearchGate: ``

Tecnologias Utilizadas üõ†Ô∏è

    Linguagem: Python 3.9+

    Vis√£o Computacional: OpenCV, MediaPipe

    Machine Learning: Scikit-learn (MLPClassifier, StandardScaler)

    Manipula√ß√£o de Dados: NumPy, Pandas

Licen√ßa ¬©Ô∏è

Este projeto est√° licenciado sob a GNU General Public License v3.0. Veja o arquivo LICENSE para mais detalhes.

Agradecimentos üôè

    Ao Professor Orientador Cec√≠lio Merlotti Rodas, pelo suporte e direcionamento acad√™mico.

    Ao Instituto Federal de Educa√ß√£o, Ci√™ncia e Tecnologia de S√£o Paulo (IFSP), pela estrutura e fomento √† pesquisa.

Este projeto foi desenvolvido com o objetivo de promover a inclus√£o e a acessibilidade atrav√©s da tecnologia. Que ele possa contribuir para quebrar barreiras de comunica√ß√£o e aproximar pessoas.

üá¨üáß üá∫üá∏ English

Methodology and How It Works

The system operates through a processing pipeline divided into three fundamental stages: data collection, model training, and real-time translation.

1. Geometric Data Collection and Representation

Instead of capturing and storing thousands of images, the system uses a webcam to detect the user's hand in real-time. The MediaPipe library is employed to identify and extract the 3D coordinates of 21 reference points (landmarks) that make up the hand's structure.  Each gesture is thus converted into a 63-dimensional numerical vector (21 points x 3 coordinates), representing its skeletal geometry. These vectors are saved into 

.csv files, creating a lightweight, precise dataset that is immune to visual noise such as lighting variations or background complexity.

2. Classification Model Training

With the geometric data collected, a machine learning model is trained to associate each coordinate vector with a letter of the alphabet. Due to the tabular and low-dimensional nature of the data, a Multi-Layer Perceptron (MLP) model was chosen, an efficient neural network architecture for this type of task. Before training, the data undergoes a standardization process to ensure that all features contribute equally to the learning process. The result is a highly optimized classifier model, trained in seconds, that learns to distinguish gestures solely based on their shape and structure.

3. Real-Time Translation

The final application integrates the previous components to provide instantaneous translation. The system captures video from the webcam, extracts the hand landmarks in each frame, applies the same normalization used during training, and feeds the coordinate vector to the trained MLP model. The model then predicts which letter the gesture corresponds to, and the result is displayed on the screen for the user. This cycle of detection, processing, and classification occurs continuously and with low latency, creating an interactive and functional communication tool.

Academic Publication

The full Final Year Project document, containing the theoretical foundation, detailed methodology, and analysis of the results, is available for viewing and download in permanent academic repositories.

    Zenodo (DOI): ``

    ResearchGate: ``

Technology Stack

    Linguagem: Python 3.9+

    Vis√£o Computacional: OpenCV, MediaPipe

    Machine Learning: Scikit-learn (MLPClassifier, StandardScaler)

    Manipula√ß√£o de Dados: NumPy, Pandas

License

This project is licensed under the GNU General Public License v3.0. See the LICENSE file for more details.

Acknowledgments

    To my advisor, Professor Cec√≠lio Merlotti Rodas, for the academic support and guidance.

    To the Federal Institute of Education, Science and Technology of S√£o Paulo (IFSP), for the infrastructure and encouragement of research.

This project was developed with the goal of promoting inclusion and accessibility through technology. May it contribute to breaking down communication barriers and bringing people closer together.
